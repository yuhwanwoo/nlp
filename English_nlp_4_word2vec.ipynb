{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "DATA_IN_PATH='./data_in/'\n",
    "TRAIN_CLEAN_DATA='train_clean.csv'\n",
    "\n",
    "train_data=pd.read_csv(DATA_IN_PATH + TRAIN_CLEAN_DATA)\n",
    "\n",
    "reviews=list(train_data['review'])\n",
    "sentiments=list(train_data['sentiment'])\n",
    "\n",
    "sentences=[]\n",
    "for review in reviews:\n",
    "    sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 시 필요한 하이퍼 파라미터\n",
    "\n",
    "# 워드 벡터 특징값 수\n",
    "num_features=300\n",
    "# 단어에 대한 최소 빈도 수\n",
    "min_word_count=40\n",
    "# 프로세스 개수\n",
    "num_workers=4\n",
    "# 컨텍스트 윈도우 크기\n",
    "context=10\n",
    "# 다운 샘플링 비율\n",
    "downsampling=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-15 09:48:16,633 : INFO : collecting all words and their counts\n",
      "2020-12-15 09:48:16,634 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-12-15 09:48:16,808 : INFO : PROGRESS: at sentence #10000, processed 1205223 words, keeping 51374 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-15 09:48:16,991 : INFO : PROGRESS: at sentence #20000, processed 2396605 words, keeping 67660 word types\n",
      "2020-12-15 09:48:17,086 : INFO : collected 74065 word types from a corpus of 2988089 raw words and 25000 sentences\n",
      "2020-12-15 09:48:17,086 : INFO : Loading a fresh vocabulary\n",
      "2020-12-15 09:48:17,126 : INFO : effective_min_count=40 retains 8160 unique words (11% of original 74065, drops 65905)\n",
      "2020-12-15 09:48:17,127 : INFO : effective_min_count=40 leaves 2627273 word corpus (87% of original 2988089, drops 360816)\n",
      "2020-12-15 09:48:17,147 : INFO : deleting the raw counts dictionary of 74065 items\n",
      "2020-12-15 09:48:17,149 : INFO : sample=0.001 downsamples 30 most-common words\n",
      "2020-12-15 09:48:17,150 : INFO : downsampling leaves estimated 2494384 word corpus (94.9% of prior 2627273)\n",
      "2020-12-15 09:48:17,166 : INFO : estimated required memory for 8160 words and 300 dimensions: 23664000 bytes\n",
      "2020-12-15 09:48:17,167 : INFO : resetting layer weights\n",
      "2020-12-15 09:48:18,533 : INFO : training model with 4 workers on 8160 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2020-12-15 09:48:19,538 : INFO : EPOCH 1 - PROGRESS: at 41.79% examples, 1048876 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-15 09:48:20,542 : INFO : EPOCH 1 - PROGRESS: at 85.63% examples, 1069347 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-15 09:48:20,857 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-15 09:48:20,858 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-15 09:48:20,867 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-15 09:48:20,874 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-15 09:48:20,875 : INFO : EPOCH - 1 : training on 2988089 raw words (2494484 effective words) took 2.3s, 1066902 effective words/s\n",
      "2020-12-15 09:48:21,880 : INFO : EPOCH 2 - PROGRESS: at 42.76% examples, 1072720 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-15 09:48:22,880 : INFO : EPOCH 2 - PROGRESS: at 85.94% examples, 1074968 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-15 09:48:23,188 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-15 09:48:23,196 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-15 09:48:23,197 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-15 09:48:23,198 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-15 09:48:23,199 : INFO : EPOCH - 2 : training on 2988089 raw words (2494189 effective words) took 2.3s, 1074614 effective words/s\n",
      "2020-12-15 09:48:24,205 : INFO : EPOCH 3 - PROGRESS: at 40.16% examples, 1004330 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-15 09:48:25,207 : INFO : EPOCH 3 - PROGRESS: at 81.42% examples, 1015424 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-15 09:48:25,622 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-15 09:48:25,630 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-15 09:48:25,632 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-15 09:48:25,644 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-15 09:48:25,645 : INFO : EPOCH - 3 : training on 2988089 raw words (2494296 effective words) took 2.4s, 1020896 effective words/s\n",
      "2020-12-15 09:48:26,650 : INFO : EPOCH 4 - PROGRESS: at 40.78% examples, 1022883 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-15 09:48:27,655 : INFO : EPOCH 4 - PROGRESS: at 82.05% examples, 1022602 words/s, in_qsize 8, out_qsize 0\n",
      "2020-12-15 09:48:28,042 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-15 09:48:28,053 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-15 09:48:28,054 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-15 09:48:28,056 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-15 09:48:28,056 : INFO : EPOCH - 4 : training on 2988089 raw words (2494526 effective words) took 2.4s, 1035253 effective words/s\n",
      "2020-12-15 09:48:29,066 : INFO : EPOCH 5 - PROGRESS: at 42.46% examples, 1060554 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-15 09:48:30,067 : INFO : EPOCH 5 - PROGRESS: at 85.30% examples, 1063975 words/s, in_qsize 7, out_qsize 0\n",
      "2020-12-15 09:48:30,377 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-15 09:48:30,386 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-15 09:48:30,389 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-15 09:48:30,398 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-15 09:48:30,399 : INFO : EPOCH - 5 : training on 2988089 raw words (2494319 effective words) took 2.3s, 1066141 effective words/s\n",
      "2020-12-15 09:48:30,399 : INFO : training on a 14940445 raw words (12471814 effective words) took 11.9s, 1051109 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model=word2vec.Word2Vec(sentences,\n",
    "                       workers=num_workers,\n",
    "                       size=num_features,\n",
    "                       min_count=min_word_count,\n",
    "                       window=context,\n",
    "                       sample=downsampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-15 09:48:36,828 : INFO : saving Word2Vec object under 300features_40minwords_10context, separately None\n",
      "2020-12-15 09:48:36,829 : INFO : not storing attribute vectors_norm\n",
      "2020-12-15 09:48:36,830 : INFO : not storing attribute cum_table\n",
      "2020-12-15 09:48:36,984 : INFO : saved 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# 모델의 하이퍼파라미터를 설정한 내용을 모델 이름에 담는다면 나중에 참고하기 좋음\n",
    "# 모델을 저장하면 Word2Vec.load()로 모델을 다시 사용 가능\n",
    "model_name=\"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(words, model, num_features):\n",
    "    # 출력 벡터 초기화\n",
    "    feature_vector=np.zeros((num_features), dtype=np.float32)\n",
    "    \n",
    "    num_words=0\n",
    "    # 어휘사전 준비\n",
    "    index2word_set=set(model.wv.index2word)\n",
    "    \n",
    "    for w in words:\n",
    "        if w in index2word_set:\n",
    "            num_words+=1\n",
    "            # 사전에 해당하는 단어에 대해 단어 벡터를 더함\n",
    "            feature_vector=np.add(feature_vector, model[w])\n",
    "    \n",
    "    # 문장의 단어 수만큼 나누어 단어 벡터의 평균값을 문장 벡터로 함\n",
    "    feature_vector=np.divide(feature_vector, num_words)\n",
    "    return feature_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(reviews, model, num_features):\n",
    "    dataset=list()\n",
    "    \n",
    "    for s in reviews:\n",
    "        dataset.append(get_features(s, model, num_features))\n",
    "    reviewFeatureVecs=np.stack(dataset)\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhwa\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs=get_dataset(sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X=test_data_vecs\n",
    "y=np.array(sentiments)\n",
    "\n",
    "RANDOM_SEED=42\n",
    "TEST_SPLIT=0.2\n",
    "\n",
    "X_train, X_eval, y_train, y_eval=train_test_split(X, y, test_size=TEST_SPLIT, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhwa\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lgs=LogisticRegression(class_weight='balanced')\n",
    "lgs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.864800\n"
     ]
    }
   ],
   "source": [
    "# 검증 데이터로 성능 측정\n",
    "print(\"Accuracy: %f\" % lgs.score(X_eval, y_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_CLEAN_DATA='test_clean.csv'\n",
    "\n",
    "test_data=pd.read_csv(DATA_IN_PATH+TEST_CLEAN_DATA)\n",
    "\n",
    "test_review=list(test_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences=[]\n",
    "for review in test_data:\n",
    "    test_sentences.append(review.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuhwa\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:13: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  del sys.path[0]\n"
     ]
    }
   ],
   "source": [
    "test_data_vecs=get_dataset(test_sentences, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-6e266c825d4c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0manswer_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'sentiment'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_predicted\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0manswer_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_OUT_PATH\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'lgs_answer.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    433\u001b[0m             )\n\u001b[0;32m    434\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 435\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minit_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    436\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36minit_dict\u001b[1;34m(data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    252\u001b[0m             \u001b[0marr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_datetime64tz_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m         ]\n\u001b[1;32m--> 254\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\construction.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    363\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"arrays must all be same length\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    366\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "DATA_OUT_PATH='./data_out/'\n",
    "\n",
    "test_predicted=lgs.predict(test_data_vecs)\n",
    "\n",
    "if not os.path.exists(DATA_OUT_PATH):\n",
    "    os.makedirs(DATA_OUT_PATH)\n",
    "    \n",
    "ids=list(test_data['id'])\n",
    "answer_dataset=pd.DataFrame({'id':ids, 'sentiment':test_predicted})\n",
    "answer_dataset.to_csv(DATA_OUT_PATH+'lgs_answer.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dataset.to_csv(DATA_OUT_PATH+'lgs_answer.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n"
     ]
    }
   ],
   "source": [
    "print(test_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001\n"
     ]
    }
   ],
   "source": [
    "print(1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
