{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본적인 라이브러리 호출\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.compat.v1 import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vocab', 'vocab_size'])\n"
     ]
    }
   ],
   "source": [
    "# 이전에 저장했던 학습에 필요한 디렉터리 설정 및 학습/평가 데이터를 불러온다.\n",
    "\n",
    "DATA_IN_PATH='./data_in/'\n",
    "DATA_OUT_PATH='./data_out/'\n",
    "INPUT_TRAIN_DATA_FILE_NAME='train_input.npy'\n",
    "LABEL_TRAIN_DATA_FILE_NAME='train_label.npy'\n",
    "\n",
    "TEST_INPUT_DATA_FILE_NAME='test_input.npy'\n",
    "\n",
    "DATA_CONFIGS_FILE_NAME='data_configs.json'\n",
    "\n",
    "train_input_data=np.load(open(DATA_IN_PATH+INPUT_TRAIN_DATA_FILE_NAME,'rb'))\n",
    "train_label_data=np.load(open(DATA_IN_PATH+LABEL_TRAIN_DATA_FILE_NAME,'rb'))\n",
    "test_input_data=np.load(open(DATA_IN_PATH+TEST_INPUT_DATA_FILE_NAME,'rb'))\n",
    "\n",
    "with open(DATA_IN_PATH+DATA_CONFIGS_FILE_NAME,'r') as f:\n",
    "    prepro_configs=json.load(f)\n",
    "    print(prepro_configs.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 파라미터 변수\n",
    "\n",
    "RNG_SEED=1234\n",
    "BATCH_SIZE=128\n",
    "NUM_EPOCHS=3\n",
    "VOCAB_SIZE=prepro_configs['vocab_size']\n",
    "EMB_SIZE=128\n",
    "VALID_SPLIT=0.2\n",
    "\n",
    "# 학습 데이터와 검증 데이터를 train_test_split 함수를 활용해 나눈다.\n",
    "train_input, eval_input, train_label, eval_label=train_test_split(train_input_data, train_label_data, test_size=VALID_SPLIT, random_state=RNG_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 학습을 위해 tf.data를 설정한다.\n",
    "\n",
    "def mapping_fn(X, Y=None):\n",
    "    input, label={'x':X},Y\n",
    "    return input,label\n",
    "\n",
    "def train_input_fn():\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((train_input, train_label))\n",
    "    dataset=dataset.shuffle(buffer_size=len(train_input))\n",
    "    dataset=dataset.batch(BATCH_SIZE)\n",
    "    dataset=dataset.map(mapping_fn)\n",
    "    dataset=dataset.repeat(count=NUM_EPOCHS)\n",
    "    \n",
    "    iterator=dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()\n",
    "\n",
    "def eval_input_fn():\n",
    "    dataset=tf.data.Dataset.from_tensor_slices((eval_input,eval_label))\n",
    "    dataset=dataset.shuffle(buffer_size=len(eval_label))\n",
    "    dataset=dataset.batch(BATCH_SIZE)\n",
    "    dataset=dataset.map(mapping_fn)\n",
    "    \n",
    "    iterator=dataset.make_one_shot_iterator()\n",
    "    \n",
    "    return iterator.get_next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "    TRAIN=mode==tf.estimator.ModeKeys.TRAIN\n",
    "    EVAL=mode==tf.estimator.ModeKeys.EVAL\n",
    "    PREDICT=mode==tf.estimator.ModeKeys.PREDICT\n",
    "    \n",
    "    # embedding layer를 선언합니다.\n",
    "    embedding_layer=keras.layers.Embedding(VOCAB_SIZE,EMB_SIZE)(features['x'])\n",
    "    \n",
    "    # embedding layer에 대한 output에 대해 dropout을 취합니다.\n",
    "    dropout_emb=keras.layers.Dropout(rate=0.5)(embedding_layer)\n",
    "    \n",
    "    ## filters=128이고 kernel_size=3,4,5다.\n",
    "    ## 길이가 3, 4, 5인 128개의 다른 필터를 생성한다. 3,4,5 gram의 효과처럼 다양한 각도에서 문장을 보는 효과가 있다.\n",
    "    ## conv1d는 (배치 크기, 길이, 채널)로 입력값을 받는데, 배치 사이즈: 문장 숫자 | 길이: 각 문장의 단어의 개수 | 채널 : 임베딩 출력 차원수임\n",
    "    \n",
    "    conv1=keras.layers.Conv1D(filters=128, kernel_size=3, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "    \n",
    "    pool1=keras.layers.GlobalMaxPool1D()(conv1)\n",
    "    \n",
    "    conv2=keras.layers.Conv1D(filters=128, kernel_size=4, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "    \n",
    "    pool2=keras.layers.GlobalMaxPool1D()(conv2)\n",
    "    \n",
    "    conv3=keras.layers.Conv1D(filters=128, kernel_size=5, padding='valid', activation=tf.nn.relu)(dropout_emb)\n",
    "    \n",
    "    pool3=keras.layers.GlobalMaxPool1D()(conv3)\n",
    "    \n",
    "    # 3,4,5gram 이후 모아주기\n",
    "    concat=keras.layers.concatenate([pool1, pool2, pool3])\n",
    "    \n",
    "    hidden=keras.layers.Dense(250, activation=tf.nn.relu)(concat)\n",
    "    dropout_hidden=keras.layers.Dropout(rate=0.5)(hidden)\n",
    "    logits=keras.layers.Dense(1, name='logits')(dropout_hidden)\n",
    "    logits=tf.squeeze(logits, axis=-1)\n",
    "    \n",
    "    # 최종적으로 학습, 검증, 평가의 단계로 나누어 활용\n",
    "    \n",
    "    if PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions={'prob':tf.nn.sigmoid(logits)})\n",
    "    \n",
    "    loss=tf.losses.sigmoid_cross_entropy(labels, logits)\n",
    "    \n",
    "    if EVAL:\n",
    "        pred=tf.nn.sigmoid(logits)\n",
    "        accuracy=tf.metrics.accuracy(labels, tf.round(pred))\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops={'acc':accuracy})\n",
    "    \n",
    "    if TRAIN:\n",
    "        global_step=tf.train.get_global_step()\n",
    "        train_op=tf.train.AdamOptimizer(0.001).minimize(loss, global_step)\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, train_op=train_op, loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'C:\\\\mywork\\\\NLP\\\\data_out/checkpoint/cnn/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From C:\\Users\\yuhwa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\training_util.py:236: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n",
      "WARNING:tensorflow:From <ipython-input-4-edaabddfde82>:14: DatasetV1.make_one_shot_iterator (from tensorflow.python.data.ops.dataset_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This is a deprecated API that should only be used in TF 1 graph mode and legacy TF 2 graph mode available through `tf.compat.v1`. In all other situations -- namely, eager mode and inside `tf.function` -- you can consume dataset elements using `for elem in dataset: ...` or by explicitly creating iterator via `iterator = iter(dataset)` and fetching its elements via `values = next(iterator)`. Furthermore, this API is not available in TF 2. During the transition from TF 1 to TF 2 you can use `tf.compat.v1.data.make_one_shot_iterator(dataset)` to create a TF 1 graph mode style iterator for a dataset created through TF 2 APIs. Note that this should be a transient state of your code base as there are in general no guarantees about the interoperability of TF 1 and TF 2 code.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\mywork\\NLP\\data_out/checkpoint/cnn/model.ckpt-471\n",
      "WARNING:tensorflow:From C:\\Users\\yuhwa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1077: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file utilities to get mtimes.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 471...\n",
      "INFO:tensorflow:Saving checkpoints for 471 into C:\\mywork\\NLP\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 471...\n",
      "INFO:tensorflow:loss = 0.0023032431, step = 471\n",
      "INFO:tensorflow:global_step/sec: 3.94878\n",
      "INFO:tensorflow:loss = 0.0010222371, step = 571 (25.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.83198\n",
      "INFO:tensorflow:loss = 0.0005900202, step = 671 (26.096 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.54691\n",
      "INFO:tensorflow:loss = 0.0003593868, step = 771 (28.194 sec)\n",
      "INFO:tensorflow:global_step/sec: 3.23966\n",
      "INFO:tensorflow:loss = 0.00031904253, step = 871 (30.867 sec)\n",
      "INFO:tensorflow:Calling checkpoint listeners before saving checkpoint 942...\n",
      "INFO:tensorflow:Saving checkpoints for 942 into C:\\mywork\\NLP\\data_out/checkpoint/cnn/model.ckpt.\n",
      "INFO:tensorflow:Calling checkpoint listeners after saving checkpoint 942...\n",
      "INFO:tensorflow:Loss for final step: 0.00025227646.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow_estimator.python.estimator.estimator.Estimator at 0x1b4c65736c8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dir=os.path.join(os.getcwd(),\"data_out/checkpoint/cnn/\")\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Estimator 객체 생성\n",
    "cnn_est=tf.estimator.Estimator(model_fn, model_dir=model_dir)\n",
    "# 학습하기\n",
    "cnn_est.train(train_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2021-01-10T21:50:10Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\mywork\\NLP\\data_out/checkpoint/cnn/model.ckpt-942\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Inference Time : 2.47937s\n",
      "INFO:tensorflow:Finished evaluation at 2021-01-10-21:50:12\n",
      "INFO:tensorflow:Saving dict for global step 942: acc = 0.8822, global_step = 942, loss = 0.44492498\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 942: C:\\mywork\\NLP\\data_out/checkpoint/cnn/model.ckpt-942\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'acc': 0.8822, 'loss': 0.44492498, 'global_step': 942}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 평가하기\n",
    "cnn_est.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_IN_PATH='./data_in/'\n",
    "DATA_OUT_PATH='./data_out/'\n",
    "TEST_INPUT_DATA='test_input.npy'\n",
    "TEST_ID_DATA='test_id.npy'\n",
    "\n",
    "test_input_data=np.load(open(DATA_IN_PATH+TEST_INPUT_DATA,'rb'))\n",
    "ids=np.load(open(DATA_IN_PATH+TEST_ID_DATA,'rb'), allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from C:\\mywork\\NLP\\data_out/checkpoint/cnn/model.ckpt-942\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "predict_input_fn=tf.estimator.inputs.numpy_input_fn(x={\"x\":test_input_data}, shuffle=False)\n",
    "\n",
    "predictions=np.array([p['prob'] for p in cnn_est.predict(input_fn=predict_input_fn)])\n",
    "\n",
    "output=pd.DataFrame(data={\"id\":list(ids), \"sentiment\":list(predictions)})\n",
    "\n",
    "output.to_csv(DATA_OUT_PATH+\"Bag_Words_model_test_cnn.csv\",index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
